<html>
<head>
  <title>Evernote Export</title>
  <basefont face="Tahoma" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="Evernote Windows/274061; Windows/6.1.7601 Service Pack 1;"/>
  <style>
    body, td {
      font-family: Tahoma;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="575"/>

<div>
<div style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;"><div>0. 机器学习中分类和预测算法的评估：</div><div><br/></div><div><ul><li>准确率</li><li>速度</li><li>强壮行</li><li>可规模性</li><li>可解释性</li></ul></div><div><br/></div><div><br/></div><div><br/></div>
1. 什么是决策树/判定树（decision tree)?
<div>     </div><div>     判定树是一个类似于流程图的树结构：其中，每个内部结点表示在一个属性上的测试，每个分支代表一个属性输出，而每个树叶结点代表类或类分布。树的最顶层是根结点。</div><div><br/></div><div>     <img src="3.1 决策树(decision tree)算法_files/c2cec3fdfc0392456a6ac4258694a4c27d1e2538.jpg" type="image/jpeg" style="cursor: default;cursor: default;cursor: default;"/></div><div><br/></div><div><br/></div><div>2.  机器学习中分类方法中的一个重要算法</div><div><br/></div><div>3.  构造决策树的基本算法                   分支                 根结点       </div><div>                                                                                             结点</div><div><img src="3.1 决策树(decision tree)算法_files/Image.png" type="image/png" style="cursor: default;cursor: default;"/>树叶</div><div><br/></div><div><br/></div><div><img src="3.1 决策树(decision tree)算法_files/Image [1].png" type="image/png" style="cursor: default;cursor: default;"/></div><div><br/></div><div><br/></div><div>     3.1 熵（entropy）概念：</div><div><br/></div><div>          信息和抽象，如何度量？</div><div>          1948年，香农提出了 ”信息熵(entropy)“的概念</div><div>          一条信息的信息量大小和它的不确定性有直接的关系，要搞清楚一件非常非常不确定的事情，或者          </div><div>          是我们一无所知的事情，需要了解大量信息==&gt;信息量的度量就等于不确定性的多少</div><div>          </div><div>          例子：猜世界杯冠军，假如一无所知，猜多少次？</div><div>          每个队夺冠的几率不是相等的</div><div>          </div><div>          比特(bit)来衡量信息的多少</div><div><br/></div><div>          <img src="3.1 决策树(decision tree)算法_files/Image [2].png" type="image/png" style="cursor: default;cursor: default;"/></div><div><br/></div><div>          <img src="3.1 决策树(decision tree)算法_files/Image [3].png" type="image/png" style="cursor: default;cursor: default;"/></div><div><br/></div><div>          变量的不确定性越大，熵也就越大</div><div>          </div><div><br/></div><div>     3.1 决策树归纳算法 （ID3）</div><div><br/></div><div>          1970-1980， J.Ross. Quinlan, ID3算法</div><div>     </div><div>          选择属性判断结点</div><div><br/></div><div>          信息获取量(Information Gain)：Gain(A) = Info(D) - Infor_A(D)</div><div>          通过A来作为节点分类获取了多少信息</div><div><br/></div><div>                </div><div>          <img src="3.1 决策树(decision tree)算法_files/Image [4].png" type="image/png" style="cursor: default;cursor: default;"/></div><div><br/></div><div><br/></div><div>          <img src="3.1 决策树(decision tree)算法_files/Image [5].png" type="image/png" style="cursor: default;cursor: default;"/></div><div><br/></div><div>          <img src="3.1 决策树(decision tree)算法_files/Image [6].png" type="image/png" style="cursor: default;cursor: default;"/></div><div>          <img src="3.1 决策树(decision tree)算法_files/Image [7].png" type="image/png" style="cursor: default;"/></div><div><br/></div><div>           类似，Gain(income) = 0.029, Gain(student) = 0.151, Gain(credit_rating)=0.048</div><div><br/></div><div>          所以，选择age作为第一个根节点</div><div><br/></div><div><br/></div><div><img src="3.1 决策树(decision tree)算法_files/Image [8].png" type="image/png" style="cursor: default;"/></div><div><br/></div><div>          重复。。。</div><div><br/></div><div><br/></div><div>          算法：</div><blockquote style="margin: 0 0 0 40px; border: none; padding: 0px;"><ul><li><span style="font-size: 10pt; color: rgb(1, 1, 1); font-family: Arial;">树以代表训练样本的单个结点开始（步骤1）。</span></li><li><span style="font-size: 10pt; color: rgb(1, 1, 1); font-family: Arial;">如果样本都在同一个类，则该结点成为树叶，并用该类标号（步骤2 和3）。</span></li><li><span style="font-size: 10pt; color: rgb(1, 1, 1); font-family: Arial;">否则，算法使用称为信息增益的基于熵的度量作为启发信息，选择能够最好地将样本分类的属</span><span style="color: rgb(1, 1, 1); font-family: Arial; font-size: 10pt;">性（步骤6）。该属性成为该结点的“测试”或“判定”属性（步骤7）。在算法的该版本中，</span></li><li><span style="font-size: 10pt; color: rgb(1, 1, 1); font-family: Arial;">所有的属性都是分类的，即离散值。连续属性必须离散化。</span></li><li><span style="font-size: 10pt; color: rgb(1, 1, 1); font-family: Arial;">对测试属性的每个已知的值，创建一个分枝，并据此划分样本（步骤8-10）。</span></li><li><span style="font-size: 10pt; color: rgb(1, 1, 1); font-family: Arial;">算法使用同样的过程，递归地形成每个划分上的样本判定树。一旦一个属性出现在一个结点上，</span><span style="color: rgb(1, 1, 1); font-family: Arial; font-size: 10pt;">就不必该结点的任何后代上考虑它（步骤13）。</span></li><li><span style="font-size: 10pt; color: rgb(1, 1, 1); font-family: Arial;">递归划分步骤仅当下列条件之一成立停止：</span></li><li><span style="font-size: 10pt; color: rgb(1, 1, 1); font-family: Arial;">(a) 给定结点的所有样本属于同一类（步骤2 和3）。</span></li><li><span style="font-size: 10pt; color: rgb(1, 1, 1); font-family: Arial;">(b) 没有剩余属性可以用来进一步划分样本（步骤4）。在此情况下，使用多数表决（步骤5）。</span></li><li><span style="font-size: 10pt; color: rgb(1, 1, 1); font-family: Arial;">这涉及将给定的结点转换成树叶，并用样本中的多数所在的类标记它。替换地，可以存放结</span></li><li><span style="font-size: 10pt; color: rgb(1, 1, 1); font-family: Arial;">点样本的类分布。</span></li><li><span style="font-size: 10pt; color: rgb(1, 1, 1); font-family: Arial;">(c) 分枝</span></li><li><font color="#010101" face="Arial"><span style="font-size:11pt">test_attribute</span></font> <font color="#010101" face="Arial" size="2"><span style="font-size:10pt">=</span></font> <font color="#010101" face="Arial"><span style="font-size:11pt">a</span></font> <font color="#010101" face="Arial"><span style="font-size:6pt">i</span></font> <font color="#010101" face="Arial" size="2"><span style="font-size:10pt">没有样本（步骤11）。在这种情况下，以</span></font> <font color="#010101" face="Arial"><span style="font-size:11pt">samples</span></font> <font color="#010101" face="Arial" size="2"><span style="font-size:10pt">中的多数类</span></font></li><li><span style="font-size: 10pt; color: rgb(1, 1, 1); font-family: Arial;">创建一个树叶（步骤12）</span></li></ul></blockquote><div><br/></div><div>               </div><div><br/></div><div><br/></div><div>     3.1 其他算法：</div><div>               C4.5:  Quinlan</div><div>               Classification and Regression Trees (CART): (L. Breiman, J. Friedman, R. Olshen, C. Stone)</div><div>               共同点：都是贪心算法，自上而下(Top-down approach)</div><div>               区别：属性选择度量方法不同： C4.5 （gain ratio), CART(gini index), ID3 (Information Gain)</div><div><br/></div><div>     3.2 如何处理连续性变量的属性？ </div><div><br/></div><div>4. 树剪枝叶 （避免overfitting)</div><div>     4.1 先剪枝</div><div>     4.2 后剪枝</div><div><br/></div><div><br/></div><div>5. 决策树的优点：</div><div>     直观，便于理解，小规模数据集有效     </div><div><br/></div><div>6. 决策树的缺点：</div><div>     处理连续变量不好</div><div>     类别较多时，错误增加的比较快</div><div>     可规模性一般（</div><div>     </div></div>
</div></body></html> 